# The aim of this exercise is to understand Lasso and Ridge regularization.
# For this we will plot the predictor vs coefficient as a horizontal bar chart. The graph will look similar to the one given below.

# Import necessary libraries
%matplotlib inline
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn.linear_model import Lasso
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures

# Read the file "Boston_housing.csv" as a Pandas dataframe
df = pd.read_csv("Boston_housing.csv")

# Select a subdataframe of predictors mentioned above*
subdf=['crim','indus','nox','rm','age','dis','rad','tax','ptratio','black','lstat']
X = df[subdf]

# Normalize the values of the dataframe 
X_norm = preprocessing.normalize(X, axis=0)

# Select medv as the response variable
y = df['medv']

### Multi-linear Regression Analysis

### edTest(test_random) ###
# Split the data into train and validation sets with 70% train data and
# random_state as 31
X_train, X_val, y_train, y_val = train_test_split(X_norm,y,train_size=0.7, random_state=31)

# Initialize a Linear Regression model
lreg = LinearRegression()

# Fit the linear model on the train data
lreg.fit(X_train,y_train)

# Predict on the validation data
y_val_pred = lreg.predict(X_val)

# Use the mean_squared_error function to compute the validation mse
mse = mean_squared_error(y_val,y_val_pred)

# Print the MSE value
print ("Multi-linear regression validation MSE is", mse)

### Obtaining the coefficients of the predictors
# Helper code to create a dictionary of the coefficients 
# along with the predictors as keys
lreg_coef = dict(zip(X.columns, np.transpose(lreg.coef_)))

# Linear regression coefficients for plotting
lreg_x = list(lreg_coef.keys())
lreg_y = list(lreg_coef.values())

### Implementing Lasso regularization
# Create a Lasso Regression model with alpha as 0.008
lasso_reg = Lasso(alpha=0.008)

# Fit the model on the train data
lasso_reg.fit(X_train,y_train)

# Predict on the validation data using the trained model
y_val_pred =lasso_reg.predict(X_val)
                             
# Calculate the validation MSE
mse_lasso = mean_squared_error(y_val,y_val_pred)

# Print the validation MSE
print ("Lasso validation MSE is", mse_lasso)

# Hhelper code to make a dictionary of the predictors 
# along with the coefficients associated with them
lasso_coef = dict(zip(X.columns, np.transpose(lasso_reg.coef_))) 

# Get the Lasso regularisation coefficients for plotting
lasso_x = list(lasso_coef.keys())
lasso_y = list(lasso_coef.values())

### Implementing Ridge regularization

# Create a Ridge Regression model with alpha as 0.008
ridgeReg = Ridge(alpha=0.008)

# Fit the model on the train data
ridgeReg.fit(X_train,y_train)

# Predict the trained model on the validation data
y_val_pred = ridgeReg.predict(X_val)

### edTest(test_mse) ###

# Calculate the validation MSE
mse_ridge = mean_squared_error(y_val,y_val_pred)

# Print he valdiation MSE
print ("Ridge validation MSE is", mse_ridge)

# Helper code to make a dictionary of the predictors 
# along with the coefficients associated with them 
ridge_coef = dict(zip(X.columns, np.transpose(ridgeReg.coef_))) 

# Ridge regularisation coefficients for plotting
ridge_x = list(ridge_coef.keys())
ridge_y = list(ridge_coef.values())

# Helper code below to visualise your results

plt.rcdefaults()

plt.barh(lreg_x,lreg_y,1.0, align='edge',color="#D3B4B4", label="Linear Regression")
plt.barh(lasso_x,lasso_y,0.75 ,align='edge',color="#81BDB2",label = "Lasso regularisation")
plt.barh(ridge_x,ridge_y,0.25 ,align='edge',color="#7E7EC0", label="Ridge regularisation")
plt.grid(linewidth=0.2)
plt.xlabel("Coefficient")
plt.ylabel("Predictors")
plt.legend(loc='best')
plt.show()

